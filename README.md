# SPARK
# Hadoop Introduction:
Hadoop is an open-source framework used to store and process large-scale data (Big Data) across a cluster of computers using distributed computing.
# Key Purpose of Hadoop
# Hadoop solves three big problems of Big Data:
1 Volume – Huge amount of data (TB, PB)
2 Velocity – High-speed data generation
3 Variety – Different types of data (structured, semi-structured, unstructured)
# Hadoop Core Components
# 1 HDFS (Hadoop Distributed File System) – Storage
Stores big files by splitting them into blocks.
Distributes blocks across many machines in a cluster.
Maintains replication for fault tolerance.
# 2 YARN (Yet Another Resource Negotiator) – Resource Management
Allocates CPU, memory, and resources to different jobs.
Schedules and manages tasks.
# 3 MapReduce – Data Processing Model
Processes data in parallel across cluster nodes.
Two phases:
1 Map → Splits work
2 Reduce → Combines output

# 1 Role of Hadoop in Distributed Computing
Hadoop enables:
# Parallel processing
Thousands of machines work together at the same time.
# Fault tolerance
If one machine fails, data is automatically recovered through replication.
# Scalability
You can add more machines to increase capacity.
# Cost efficiency
Runs on inexpensive commodity hardware.
# Handling any type of data
Text, images, videos, logs, social media, IoT, etc.

# 2 Historical Context and Evolution:
